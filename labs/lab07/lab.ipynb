{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765de4e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2078e5",
   "metadata": {},
   "source": [
    "# Lab 7 ‚Äì Regular Expressions\n",
    "\n",
    "## DSC 80, Spirng 2023\n",
    "\n",
    "### Due Date: Monday, May 22nd at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4131e90",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Welcome to the seventh lab assignment in DSC 80 this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2023-wi).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- **To ensure that all of your work to be submitted is in `lab.py`, we've provided an additional uneditable notebook, called `lab-validation.ipynb`, that contains only the tests and their setup. Make sure you are able to run it top-to-bottom without error before submitting!**\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7772f3",
   "metadata": {},
   "source": [
    "## Question 1 ‚Äì Practice with Regular Expressions üõ†\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>: The functions in this question all have doctests in their docstrings. The doctests constitute the public tests; we will still run hidden tests on each of your functions. <b>Please don't change any of the docstrings!</b>\n",
    "</div>\n",
    "\n",
    "Regular expressions can be tricky, and the best way to gain familiarity with them is through lots of practice. In this question, you will work through ten exercises, each of which requires you to write a regular expression that matches strings that satisfy certain criteria. Make sure to take a close look at the doctests for each function in `lab.py`, as they provide useful guidance for the types of strings you should and shouldn't match.\n",
    "\n",
    "***Notes:*** \n",
    "- Make sure to refer to the [Regular Expression Resources](https://dsc80.com/resources/#regular-expressions) posted on the course website. In particular, we recommend having [regex101.com](https://regex101.com/) open while working, along with the [cheat sheet](https://dsc80.com/resources/other/berkeley-regex-reference.pdf).\n",
    "\n",
    "- Each exercise has a star rating, between 1 (‚≠êÔ∏è) and 3 (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è) stars, indicating its difficulty level (1 being the easiest, 3 being the hardest). If you are spending lots of time on 1-star exercises, take a close look at the syntax from lecture, as there is probably an easier way of writing the necessary pattern!\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 1 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that have `'['` as the third character and `']'` as the sixth character.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 2 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that are phone numbers that start with `'(858)'` and follow the format `'(xxx) xxx-xxxx'` (`'x'` represents a digit).\n",
    "\n",
    "***Note:*** There is a space between `'(xxx)'` and `'xxx-xxxx'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 3 (‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings that:\n",
    "- are between 6 and 10 characters long (inclusive),\n",
    "- contain only alphanumeric characters, whitespace and `'?'`, and\n",
    "- end with `'?'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 4 (‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "Write a regular expression that matches strings with exactly two `'$'`, one of which is at the start of the string, such that:\n",
    "- the characters between the two `'$'` can be anything (including nothing) except the lowercase letters `'a'`, `'b'`, and `'c'`, (and `'$'`), and\n",
    "- the characters after the second `'$'` can only be the **lowercase or uppercase** letters `'a'`/`'A'`, `'b'`/`'B'`, and `'c'`/`'C'`, with every `'a'`/`'A'` before every `'b'`/`'B'`, and every `'b'`/`'B'` before every `'c'`/`'C'`. There must be at least one `'a'` or `'A'`, at least one `'b'` or `'B'`, and at least one `'c'` or `'C'`.\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 5 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that represent valid Python file names, including the extension. \n",
    "\n",
    "***Note:*** For simplicity, assume that file names only contain letters, numbers, and underscores (`'_'`).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 6 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that:\n",
    "- are made up of only lowercase letters and exactly one underscore (`'_'`), and\n",
    "- have at least one lowercase letter on both sides of the underscore.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 7 (‚≠êÔ∏è)\n",
    "Write a regular expression that matches strings that start with and end with an underscore (`'_'`).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 8 (‚≠êÔ∏è)\n",
    "\n",
    "Apple serial numbers are strings of length 1 or more that are made up of any characters, other than\n",
    "- the uppercase letter `'O'`, \n",
    "- the lowercase letter `'i`', and \n",
    "- the number `'1'`.\n",
    "\n",
    "Write a regular expression that matches strings that are valid Apple serial numbers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 9 (‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "ID numbers are formatted as `'SC-NN-CCC-NNNN'`, where \n",
    "- SC represents state code in uppercase (e.g. `'CA'`),\n",
    "- NN represents a number with 2 digits (e.g. `'98'`),\n",
    "- CCC represents a three letter city code in uppercase (e.g. `'SAN'`), and\n",
    "- NNNN represents a number with 4 digits (e.g. `'1024'`).\n",
    "\n",
    "Write a regular expression that matches strings that are ID numbers corresponding to the cities of `'SAN'` or `'LAX'`, or the state of `'NY'`. Assume that there is only one city named `'SAN'` and only one city named `'LAX'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exercise 10 (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è)\n",
    "\n",
    "Write a function named `match_10` that takes in a string and:\n",
    "- converts the string to lowercase,\n",
    "- removes all non-alphanumeric characters (i.e. removes everything that is not in the `\\w` character class), and the letter `'a'`, and\n",
    "- returns a list of every **non-overlapping** three-character substring in the remaining string, starting from the beginning of the string.\n",
    "   \n",
    "For instance, consider the following doctest:\n",
    "\n",
    "```py\n",
    ">>> match_10('Ab..DEF')\n",
    "['bde']\n",
    "```\n",
    "\n",
    "Here's how `match_10` should process `'Ab..DEF'`:\n",
    "\n",
    "1. Convert to lowercase: `'ab..def'`.\n",
    "2. Remove non-alphanumeric characters and the letter `'a'`: `'bdef'`.\n",
    "3. Starting from the beginning of the string, there is only a single non-overlapping three character substring: `'bde'`. Hence, we return `['bde']`.\n",
    "\n",
    "***Note:*** Perform your operations in the exact order described above, otherwise your code may not pass all the tests. Don't use a `for`-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854442f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8458c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696850f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579697c5",
   "metadata": {},
   "source": [
    "## Question 2 ‚Äì Capturing Groups in Regular Expressions üì°\n",
    "\n",
    "The dataset stored in `data/messy.txt` contains personal information from a fictional website that a user scraped from web server logs. Within this dataset, there are four fields that are of interest to you:\n",
    "1. Email Addresses (assume they are alphanumeric usernames and domain names)\n",
    "2. [Social Security Numbers](https://en.wikipedia.org/wiki/Social_Security_number#Structure)\n",
    "3. Bitcoin Addresses (alphanumeric strings of long length)\n",
    "4. Street Addresses\n",
    "\n",
    "Complete the implementation of the function `extract_personal`, which takes in a string containing the contents of a server log file (like `open('data/messy.txt').read()`) and returns a **tuple of four separate lists** containing values of the 4 pieces of information listed above (in the order listed above). Do **not** keep empty values.\n",
    "\n",
    "***Note:*** Since this data is messy, your function will be allowed to miss ~5% of the records in each list. Good spot checking using certain useful substrings (e.g. `'@'` for emails) should help assure correctness! Your function will be tested on a sample of the file `messy.txt`.\n",
    "\n",
    "***Hint:*** There are multiple \"delimiters\" in use in the file; there are few enough of them that you can safely determine what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with extract_personal using the file s below\n",
    "fp = os.path.join('data', 'messy.txt')\n",
    "s = open(fp, encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01268b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b06ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "test_fp = os.path.join('data', 'messy.test.txt')\n",
    "test_s = open(test_fp, encoding='utf8').read()\n",
    "emails, ssn, bitcoin, addresses = extract_personal(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99905d39",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5deed",
   "metadata": {},
   "source": [
    "## Question 3 ‚Äì TF-IDF üìä\n",
    "\n",
    "The dataset `data/reviews.txt` contains [Amazon reviews](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews/) for ~200k phones and phone accessories. The dataset has already been \"cleaned\" for you. In this question, you will complete the implementation of a function, which takes in the reviews dataset as a Series (with one entry per review) as well as a single review, and returns the word that \"best summarizes the single review\" using TF-IDF.\n",
    "\n",
    "To do so, implement the two functions below.\n",
    "\n",
    "#### `tfidf_data`\n",
    "You will first need to decompose the review into individual words, count their frequencies, and calculate the TF-IDF score for each word. Complete the implementation of the function `tfidf_data`, which takes in the reviews data as a Series (`reviews_ser`) and a single review (`review`) from that Series and returns a DataFrame indexed by the words in `review` with four columns:\n",
    "- `'cnt'`: the number of times each word is found in the review (telling how frequently a word is used in the `review`)\n",
    "- `'tf'`: the term frequency for each word (giving an idea of how important each word relative to the other words in the `review`)\n",
    "- `'idf'`: the inverse document frequency for each word (helping understand how important each word is to the whole `reviews_ser`)\n",
    "- `'tfidf'` the TF-IDF for each word (measuring how important each word is to `review`, taking into account both the frequency of the word in the `review` and across `reviews_ser`)\n",
    "\n",
    "You may use a `for`-loop. The words in the outputted DataFrame may appear in any order.\n",
    "\n",
    "***Hint:*** You may need to use the [`'\\b'` character](https://www.regular-expressions.info/wordboundaries.html) somewhere.\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### `relevant_word`\n",
    "\n",
    "Complete the implementation of the function `tfidf_data`, which takes in the DataFrame that `tfidf_data` returns and returns the word that \"best summarizes\" the review. If there are multiple \"best\" summary words, return any one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with tfidf_data using reviews_ser and review below \n",
    "fp = os.path.join('data', 'reviews.txt')\n",
    "reviews_ser = pd.read_csv(fp, header=None).squeeze(\"columns\")\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c649a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ff1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "fp = os.path.join('data', 'reviews.txt')\n",
    "reviews_ser = pd.read_csv(fp, header=None).squeeze(\"columns\")\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()\n",
    "q3_tfidf = tfidf_data(reviews_ser, review)\n",
    "\n",
    "try:\n",
    "    q3_rel = relevant_word(q3_tfidf)\n",
    "except:\n",
    "    q3_rel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39dfd7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74b17e",
   "metadata": {},
   "source": [
    "## Questions 4 and 5 ‚Äì Tweet Analysis üê•\n",
    "\n",
    "The dataset `data/ira.csv` contains tweets tagged by Twitter as likely being posted by the [Internet Research Agency](https://en.wikipedia.org/wiki/Internet_Research_Agency), the tweet factory facing allegations for attempting to influence US political elections.\n",
    "\n",
    "Questions 4 and 5 will focus on the following:\n",
    "- Question 4: Look at the hashtags present in the text and trends in their makeup.\n",
    "- Question 5: Prepare the dataset for modeling by creating features out of the text fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc9a8f",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Hashtags #Ô∏è‚É£\n",
    "\n",
    "You may assume that a hashtag is any string without whitespace that immediately follows a `'#'`.\n",
    "\n",
    "#### `hashtag_list`\n",
    "\n",
    "Complete the implementation of the function `hashtag_list`, which takes in a Series of tweet texts and returns a Series containing a list of hashtags present in each tweet's text. If a tweet's text doesn't contain a hashtag, the Series should contain an empty list for that tweet. Don't include the `'#'` symbol in the lists that are returned.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `most_common_hashtag`\n",
    "\n",
    "Complete the implementation of the function `most_common_hashtag`, which takes in a Series of hashtag lists (as is outputted by `hashtag_list`) and returns a Series consisting of a single hashtag per tweet: \n",
    "- If the tweet's text has no hashtags, the entry should in the output Series should be `NaN`.\n",
    "- If the tweet's text has one distinct hashtag, the entry in the output Series should be that hashtag.\n",
    "- If the tweet's text has more than one hashtag, the entry in the output Series should be the most common hashtag **in the tweet's text** with respect to **the whole input Series**. If there is a tie for the most common, any of the most common can be returned.\n",
    "    - For example, if the input Series was `pd.Series([[2], [2], [3, 2, 3]])`, the output would be `pd.Series([2, 2, 2])`. Even though `3` was more common in the third list than `2`, `2` is more common than `3` among all hashtags in the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The public tests don't test your work on the `ira` data,\n",
    "# but the hidden tests do.\n",
    "# So, make sure to thoroughly test your work yourself!\n",
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2de014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fd4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244649e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e29cfb",
   "metadata": {},
   "source": [
    "### Question 5 ‚Äì Features üìã\n",
    "\n",
    "Now, create a DataFrame of features from the `ira` data.  That is, create a function `create_features` that takes in a DataFrame `ira` that has just a single column, `'text'`, and returns a DataFrame with the same index as `ira` (i.e. the rows correspond to the same tweets) and the following columns:\n",
    "* `'num_hashtags'`, the number of hashtags present in the tweet.\n",
    "* `'mc_hashtags'`, the most common hashtag associated to the tweet (using the result of `most_common_hashtag` from Question 4).\n",
    "* `'num_tags'`, the number of tags the tweet has (look for the presence of `'@'`).\n",
    "* `'num_links'`, the number of hyperlinks present in the tweet.\n",
    "    - A hyperlink is a string starting with `'http://'` or `'https://'`, not followed by whitespaces.\n",
    "* `'is_retweet'`, a Boolean describing whether the tweet is a retweet. A retweet is a tweet that **begins** with `'RT'`.\n",
    "* `'text'`, a version of the tweet's text that is cleaned according to the following steps, **in this exact order**:\n",
    "    1. All meta-information above (retweet info, tags, hyperlinks, and hashtags) should be replaced with a single space.\n",
    "    2. Everything other than letters, numbers, and spaces should be replaced with a single space.\n",
    "    3. All letters should be lowercase.\n",
    "    4. All words should be separated by exactly one space, and leading/trailing whitespace should be removed (stripped).\n",
    "    \n",
    "The columns in the outputted DataFrame must be in the order `['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']`. (Remember, the DataFrame that `create_features` is called on only has a single column, `'text'`.)\n",
    "\n",
    "***Notes:***\n",
    "- It's a good idea to make helper function for each column.\n",
    "- The `\\w` character class in regex **does not** refer to letters, numbers, and spaces (or even just letters and numbers). As such, you can't use it here!\n",
    "- `create_features` will take a while to run on the entire dataset ‚Äì test it on a small sample first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The doctests/public tests don't test your work on the `ira` data,\n",
    "# but the hidden tests do.\n",
    "# So, make sure to thoroughly test your work yourself!\n",
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a509ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23d2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "# (yes, we know it says \"hidden\" ‚Äì there are still truly hidden tests in this question)\n",
    "fp_hidden = 'data/ira_test.csv'\n",
    "ira_hidden = pd.read_csv(fp_hidden, header=None)\n",
    "text_hidden = ira_hidden.iloc[:, -1:]\n",
    "text_hidden.columns = ['text']\n",
    "\n",
    "test_hidden = create_features(text_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d6c4d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0792a0",
   "metadata": {},
   "source": [
    "## Congratulations! You're done with Lab 7! üèÅ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To verify that all of your work is indeed in `lab.py`, and that you didn't accidentally implement a function in this notebook and not in `lab.py`, we've included another notebook in the lab folder, called `lab-validation.ipynb`. `lab-validation.ipynb` is a version of this notebook with only the `grader.check` cells and the code needed to set up the tests. \n",
    "\n",
    "### **Go to `lab-validation.ipynb`, and go to Kernel > Restart & Run All.** This will check if all `grader.check` test cases pass using just the code in `lab.py`.\n",
    "\n",
    "Once you're able to pass all test cases in `lab-validation.ipynb`, including the call to `grader.check_all()` at the very bottom, then you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30451973",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12cfe8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import doctest\n>>> doctest.run_docstring_examples(match_1, {'match_1': match_1})\n>>> doctest.run_docstring_examples(match_2, {'match_2': match_2})\n>>> doctest.run_docstring_examples(match_3, {'match_3': match_3})\n>>> doctest.run_docstring_examples(match_4, {'match_4': match_4})\n>>> doctest.run_docstring_examples(match_5, {'match_5': match_5})\n>>> doctest.run_docstring_examples(match_6, {'match_6': match_6})\n>>> doctest.run_docstring_examples(match_7, {'match_7': match_7})\n>>> doctest.run_docstring_examples(match_8, {'match_8': match_8})\n>>> doctest.run_docstring_examples(match_9, {'match_9': match_9})\n>>> doctest.run_docstring_examples(match_10, {'match_10': match_10})\n",
         "failure_message": "doctests",
         "hidden": false,
         "locked": false,
         "points": 10
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> emails[0] == 'test@test.com'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> ssn[0] == '423-01-9575'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bitcoin[0] == '1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> addresses[0] == '530 High Street'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3_tfidf['cnt'].sum() == 85\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 'before' in q3_tfidf.index\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q3_tfidf.loc['a', 'cnt'] == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q3_tfidf.loc['phone', 'idf'], 0.820703, atol=0.5)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> q3_rel in q3_tfidf.index\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n>>> test = pd.DataFrame(testdata, columns=['text'])['text']\n>>> out = hashtag_list(test)\n>>> out.iloc[0] == ['NLP', 'NLP1', 'NLP1']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n>>> test = hashtag_list(pd.DataFrame(testdata, columns=['text'])['text'])\n>>> most_common_hashtag(test).iloc[0] == 'NLP1'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = pd.Series([[2], [2], [3, 2, 3]])\n>>> (most_common_hashtag(out) == 2).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n>>> test = pd.DataFrame(testdata, columns=['text'])\n>>> out = create_features(test)\n>>> anscols = ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']\n>>> ansdata = [['text cleaning is cool', 3, 'NLP1', 1, 1, True]]\n>>> ans = pd.DataFrame(ansdata, columns=anscols)\n>>> (out == ans).all().all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> test_hidden.columns.tolist() == ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 400 <= (test_hidden['num_tags'] == 0).sum() <= 450\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> 450 <= (test_hidden['num_links'] == 0).sum() <= 550\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
